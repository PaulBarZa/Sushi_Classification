{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn\n",
    "import joblib\n",
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid tensorflow warnings and info messages about my poor bad CPU\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep learning part**\n",
    "\n",
    "Note: See [here](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) for dataset directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path and names\n",
    "training_set_path = \"training_set/\"\n",
    "validation_set_path = \"validation_set/\"\n",
    "\n",
    "if not os.path.exists(training_set_path):\n",
    "    os.makedirs(training_set_path)\n",
    "    \n",
    "if not os.path.exists(validation_set_path):\n",
    "    os.makedirs(validation_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model & data parameters\n",
    "model_name = 'retrain_mobilenet_v4'\n",
    "retrain_convolution = False\n",
    "num_classes = 50\n",
    "\n",
    "image_size = 192 #in pixels\n",
    "num_classes = 50\n",
    "validation_size = 0.2\n",
    "input_shape = (image_size, image_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "optimizer = keras.optimizers.Adam(1e-4) #learning_rate=0.001 (default value)\n",
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset and normalize data to the range [-1, 1]\n",
    "X, y = load_data((image_size, image_size))\n",
    "X /= 127.5\n",
    "X -= 1\n",
    "# Split Training/Testing and validation test\n",
    "X, X_validation, y, y_validation = train_test_split(X, y, test_size=validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call back\n",
    "#early_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    #width_shift_range=0.2,\n",
    "    #height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "datagen.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNet\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "pretrain = MobileNetV2(weights=\"imagenet\",  alpha=0.5, input_shape = input_shape, include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ResNet\n",
    "from tensorflow.keras.applications.resnet import ResNet101\n",
    "\n",
    "pretrain = ResNet101(weights=\"imagenet\", input_shape = input_shape, include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet\n",
    "from tensorflow.keras.applications.densenet import DenseNet201\n",
    "\n",
    "pretrain = DenseNet201(weights=\"imagenet\", input_shape = input_shape, include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Network\n",
    "\n",
    "M = keras.Sequential()\n",
    "\n",
    "M.add(layers.Conv2D(32, (3, 3), input_shape = input_shape, activation = 'relu'))\n",
    "M.add(layers.MaxPooling2D(pool_size = (2, 2)))\n",
    "M.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "M.add(layers.MaxPooling2D(pool_size = (2, 2)))\n",
    "M.add(layers.Flatten())\n",
    "\n",
    "M.add(layers.Dense(64, activation = 'relu'))\n",
    "M.add(layers.Dense(num_classes, activation = 'softmax'))\n",
    "model = M\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the selected model (if not a custom one, else do not run this part)\n",
    "if not retrain_convolution:\n",
    "    for layer in pretrain.layers:\n",
    "        layer.trainable = False\n",
    "    pretrain.layers[0].trainable = False\n",
    "    \n",
    "pretrain_out = pretrain.output\n",
    "\n",
    "M = layers.MaxPooling2D()(pretrain_out)\n",
    "M = layers.Flatten()(M)\n",
    "M = layers.Dropout(0.5)(M)\n",
    "M = layers.Dense(num_classes, activation=\"softmax\")(M)\n",
    "\n",
    "#Compile the model\n",
    "model = keras.Model(inputs=pretrain.input, outputs=M)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model without tensorflow data augmentation\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\", \"top_k_categorical_accuracy\"])\n",
    "history = model.fit(X, y, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with tensorflow data augmentation\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\", \"top_k_categorical_accuracy\"])\n",
    "history = model.fit(\n",
    "         datagen.flow(X, y, batch_size=batch_size, subset='training'),\n",
    "         validation_data=datagen.flow(X, y,batch_size=16, subset='validation'),\n",
    "         epochs=epochs, \n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation of the model\n",
    "scores = model.evaluate(X_validation, y_validation, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict validation set\n",
    "prediction = np.argmax(model.predict(X_validation), axis=-1)\n",
    "y_prediction = np.argmax(y_validation, axis=-1)\n",
    "\n",
    "# Confusion matrix of this validation\n",
    "cm = metrics.confusion_matrix(y_prediction, prediction)\n",
    "plt.figure(figsize = (10,7))\n",
    "seaborn.heatmap(cm, annot=True, linewidths=1)\n",
    "plt.savefig(\"dl_confusion_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title(\"Retrain \" + model_name + \" (alpha= 0.5) top 1 accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation top 5 values\n",
    "plt.plot(history.history['top_k_categorical_accuracy'])\n",
    "plt.plot(history.history['val_top_k_categorical_accuracy'])\n",
    "plt.title(\"Retrain \" + model_name + \" (alpha= 0.5) top 5 accuracy\")\n",
    "plt.ylabel(\"Top 5 categorical accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Retrain \" + model_name + \" (alpha= 0.5) loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = './models/' + model_name + '.keras'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training and validation set for Random Forest\n",
    "training_set_model_path = training_set_path + model_name\n",
    "validation_set_model_path = validation_set_path + model_name\n",
    "\n",
    "if not os.path.exists(training_set_model_path):\n",
    "    os.makedirs(training_set_model_path)\n",
    "    \n",
    "if not os.path.exists(validation_set_model_path):\n",
    "    os.makedirs(validation_set_model_path)\n",
    "    \n",
    "np.save(training_set_model_path + \"/X_training.npy\", X)\n",
    "np.save(training_set_model_path + \"/y_training.npy\", y)\n",
    "np.save(validation_set_model_path + \"/X_validation.npy\", X_validation)\n",
    "np.save(validation_set_model_path + \"/y_validation.npy\", y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**\n",
    "This part is for add a random forest after our convolutif part.\n",
    "\n",
    "- 1. We extract important features informations with convolutif part of our trained model\n",
    "- 2. We train our random forest on the same training set\n",
    "- 3. Test accuracy on the same validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path and names\n",
    "dl_model_name = \"retrain_mobilenet_v4\"\n",
    "rf_model_path = \"./random_forest.joblib\"\n",
    "training_set_path = \"training_set/\"\n",
    "validation_set_path = \"validation_set/\"\n",
    "\n",
    "training_set_model_path = training_set_path + dl_model_name\n",
    "validation_set_model_path = validation_set_path + dl_model_name\n",
    "\n",
    "# Layers to remove from our deep learning model to only get convolutif part\n",
    "layers_to_remove = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest parameters\n",
    "n_estimators = 320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Extract important features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the deep learning model\n",
    "model = keras.models.load_model('./models/' + dl_model_name + '.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the CNN (without the dense part)\n",
    "feature_extractor = keras.Model(model.input, model.layers[-layers_to_remove].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Training part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set\n",
    "X = np.load(training_set_model_path + \"/X_training.npy\")\n",
    "y = np.load(training_set_model_path + \"/y_training.npy\")\n",
    "\n",
    "# Transform y for Random Forest (need to be indice of the class as target value, not one hot encoded)\n",
    "y_encoded = []\n",
    "for y_ in y:\n",
    "    y_encoded.append(np.argmax(y_))\n",
    "    \n",
    "# Try to save RAM space :)\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init random forest\n",
    "RF_model = RandomForestClassifier(n_estimators = n_estimators)\n",
    "\n",
    "# Extract features\n",
    "X_for_RF = feature_extractor.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the random forest\n",
    "RF_model.fit(X_for_RF, y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the random forest model\n",
    "joblib.dump(RF_model, rf_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Validation part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation set\n",
    "X_validation = np.load(validation_set_model_path+ \"/X_validation.npy\")\n",
    "y_validation = np.load(validation_set_model_path + \"/y_validation.npy\")\n",
    "\n",
    "# Transform y validaton for Random Forest (need to be indice of the class as target value, not one hot encoded)\n",
    "y_validation_encoded = []\n",
    "for y_ in y_validation:\n",
    "     y_validation_encoded.append(np.argmax(y_))\n",
    "        \n",
    "# Try to save RAM space\n",
    "y_validation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the random forest model (if needed)\n",
    "RF_model = joblib.load(rf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on validation test\n",
    "X_validation_features = feature_extractor(X_validation)\n",
    "predicted_values = RF_model.predict(X_validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Top 1 accuracy: \", metrics.accuracy_score(y_validation_encoded, predicted_values))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = metrics.confusion_matrix(y_validation_encoded, predicted_values)\n",
    "seaborn.heatmap(cm, annot=True, linewidths=1)\n",
    "plt.savefig(\"RF_DL_confusion_matrix\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}